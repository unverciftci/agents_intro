{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQD2yOtkzWPcOEEAtJnsMl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unverciftci/agents_intro/blob/main/Notebook_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI with Memory\n",
        "### We want to build an agent that remembers conversations and learns facts\n"
      ],
      "metadata": {
        "id": "XbQTX0YkEcqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model, chat function and tools as before"
      ],
      "metadata": {
        "id": "PkkojSJxMa_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load AI (same as previous tutorials)\n",
        "print(\"Loading AI...\")\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"‚úÖ AI ready!\")\n",
        "\n",
        "# Step 1: Tools from Tutorial 2\n",
        "def calculator(expression):\n",
        "    \"\"\"Calculator tool\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return f\"Result: {result}\"\n",
        "    except:\n",
        "        return \"Error: Invalid calculation\"\n",
        "\n",
        "def get_weather(city):\n",
        "    \"\"\"Weather tool\"\"\"\n",
        "    weather_data = {\n",
        "        \"paris\": \"Sunny, 22¬∞C\",\n",
        "        \"london\": \"Rainy, 15¬∞C\",\n",
        "        \"tokyo\": \"Cloudy, 18¬∞C\",\n",
        "        \"new york\": \"Windy, 20¬∞C\"\n",
        "    }\n",
        "    return weather_data.get(city.lower(), \"Weather data not available\")\n",
        "\n",
        "def remember_fact(fact):\n",
        "    \"\"\"Tool to remember facts\"\"\"\n",
        "    try:\n",
        "        if \" is \" in fact:\n",
        "            key, value = fact.split(\" is \", 1)\n",
        "            memory.learn_fact(key.strip(), value.strip())\n",
        "            return f\"I'll remember that {key.strip()} is {value.strip()}\"\n",
        "        else:\n",
        "            memory.learn_fact(\"note\", fact)\n",
        "            return f\"I'll remember: {fact}\"\n",
        "    except:\n",
        "        return \"I couldn't understand what to remember\"\n",
        "\n",
        "TOOLS = {\n",
        "    \"calculator\": calculator,\n",
        "    \"get_weather\": get_weather,\n",
        "    \"remember\": remember_fact\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Tools ready: calculator, weather, remember\")\n",
        "\n",
        "# Step 2: AI function\n",
        "def ask_ai(prompt):\n",
        "    \"\"\"Send prompt to AI\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=130,\n",
        "            temperature=0.3,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "    return tokenizer.decode(output_ids, skip_special_tokens=True).strip()"
      ],
      "metadata": {
        "id": "tMFIxnIzFDSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our agent's memory system\n"
      ],
      "metadata": {
        "id": "osS4-W9tFIwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create memory system\n",
        "class AgentMemory:\n",
        "    \"\"\"Simple memory system for the agent\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.conversation_history = []\n",
        "        self.learned_facts = {}\n",
        "\n",
        "    def add_conversation(self, user_message, agent_response):\n",
        "        \"\"\"Remember this conversation\"\"\"\n",
        "        self.conversation_history.append({\n",
        "            'timestamp': datetime.now().strftime(\"%H:%M\"),\n",
        "            'user': user_message,\n",
        "            'agent': agent_response\n",
        "        })\n",
        "\n",
        "        # Keep only last 8 conversations to avoid token limits\n",
        "        if len(self.conversation_history) > 8:\n",
        "            self.conversation_history = self.conversation_history[-8:]\n",
        "\n",
        "    def learn_fact(self, key, value):\n",
        "        \"\"\"Learn a new fact\"\"\"\n",
        "        self.learned_facts[key] = value\n",
        "        print(f\"üß† Learned: {key} = {value}\")\n",
        "\n",
        "    def get_context(self):\n",
        "        \"\"\"Build context from memory\"\"\"\n",
        "        context = \"\"\n",
        "\n",
        "        # Add learned facts\n",
        "        if self.learned_facts:\n",
        "            context += \"What I know about you:\\n\"\n",
        "            for key, value in self.learned_facts.items():\n",
        "                context += f\"- {key}: {value}\\n\"\n",
        "            context += \"\\n\"\n",
        "\n",
        "        # Add recent conversations\n",
        "        if self.conversation_history:\n",
        "            context += \"Recent conversation:\\n\"\n",
        "            for conv in self.conversation_history[-3:]:  # Last 3 only\n",
        "                context += f\"[{conv['timestamp']}] You: {conv['user']}\\n\"\n",
        "                context += f\"[{conv['timestamp']}] Me: {conv['agent']}\\n\"\n",
        "            context += \"\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "# Create memory instance\n",
        "memory = AgentMemory()\n",
        "print(\"‚úÖ Memory system ready\")"
      ],
      "metadata": {
        "id": "2ka3b1HOFRwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our smart agent with memory"
      ],
      "metadata": {
        "id": "JkCsq34HFU5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Smart agent with memory\n",
        "def smart_agent(user_message):\n",
        "    \"\"\"Agent with memory and tools\"\"\"\n",
        "\n",
        "    # Get memory context\n",
        "    context = memory.get_context()\n",
        "\n",
        "    # System prompt with memory and tools\n",
        "    system_prompt = f\"\"\"You are a helpful AI assistant with memory and tools.\n",
        "\n",
        "{context}\n",
        "\n",
        "Available tools:\n",
        "- TOOL:calculator:expression (for math)\n",
        "- TOOL:get_weather:city (for weather)\n",
        "- TOOL:remember:fact (to remember something about the user)\n",
        "\n",
        "Examples:\n",
        "User: What's 25 + 17?\n",
        "AI: I'll calculate that. TOOL:calculator:25+17\n",
        "\n",
        "User: My name is Sarah\n",
        "AI: Nice to meet you! TOOL:remember:name is Sarah\n",
        "\n",
        "User: What's the weather in Tokyo?\n",
        "AI: Let me check that for you. TOOL:get_weather:tokyo\n",
        "\n",
        "Current message: {user_message}\n",
        "Response:\"\"\"\n",
        "\n",
        "    print(f\"üë§ User: {user_message}\")\n",
        "\n",
        "    # Get AI response\n",
        "    ai_response = ask_ai(system_prompt)\n",
        "    print(f\"ü§ñ AI: {ai_response}\")\n",
        "\n",
        "    # Handle tool calls\n",
        "    if \"TOOL:\" in ai_response:\n",
        "        tool_match = re.search(r'TOOL:(\\w+):(.+)', ai_response)\n",
        "        if tool_match:\n",
        "            tool_name = tool_match.group(1)\n",
        "            tool_input = tool_match.group(2).strip()\n",
        "\n",
        "            print(f\"üõ†Ô∏è Using tool: {tool_name}({tool_input})\")\n",
        "\n",
        "            if tool_name in TOOLS:\n",
        "                tool_result = TOOLS[tool_name](tool_input)\n",
        "                print(f\"‚úÖ Tool result: {tool_result}\")\n",
        "\n",
        "                # Get final response\n",
        "                final_prompt = system_prompt + ai_response + f\"\\nTool result: {tool_result}\\nFinal response:\"\n",
        "                final_response = ask_ai(final_prompt)\n",
        "\n",
        "                # Remember this conversation\n",
        "                memory.add_conversation(user_message, final_response)\n",
        "\n",
        "                return f\"{ai_response}\\nTool result: {tool_result}\\n{final_response}\"\n",
        "\n",
        "    # Remember normal conversations too\n",
        "    memory.add_conversation(user_message, ai_response)\n",
        "    return ai_response"
      ],
      "metadata": {
        "id": "LBCsq5OSFWOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test memory"
      ],
      "metadata": {
        "id": "wb4dR1sQFdJ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeluLMOzOS2S"
      },
      "outputs": [],
      "source": [
        "# Step 5: Test memory\n",
        "print(\"\\nüß™ Testing memory:\")\n",
        "\n",
        "test_conversation = [\n",
        "    \"Hi! My name is Alex\",\n",
        "    \"I'm 25 years old\",\n",
        "    \"I live in Paris\",\n",
        "    \"What's my name?\",\n",
        "    \"How old am I?\",\n",
        "    \"What's the weather where I live?\"\n",
        "]\n",
        "\n",
        "for message in test_conversation:\n",
        "    print(f\"\\n--- Message ---\")\n",
        "    response = smart_agent(message)\n",
        "    print(f\"Final response: {response}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Step 6: Show memory contents\n",
        "print(\"\\nüß† What's in memory:\")\n",
        "print(\"\\nüìö Learned Facts:\")\n",
        "for key, value in memory.learned_facts.items():\n",
        "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
        "\n",
        "print(\"\\nüí¨ Recent Conversations:\")\n",
        "for conv in memory.conversation_history[-3:]:\n",
        "    print(f\"  [{conv['timestamp']}] User: {conv['user']}\")\n",
        "    print(f\"  [{conv['timestamp']}] Agent: {conv['agent']}\")\n",
        "\n",
        "# Step 7: Interactive mode\n",
        "def chat_with_memory():\n",
        "    \"\"\"Interactive chat with memory-enabled agent\"\"\"\n",
        "    print(\"\\nüß† Chat with your smart agent!\")\n",
        "    print(\"Tell it about yourself and test its memory!\")\n",
        "    print(\"Type 'memory' to see what it remembers\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nüë§ You: \")\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "        elif user_input.lower() == 'memory':\n",
        "            print(\"\\nüß† Memory contents:\")\n",
        "            print(\"Facts:\", memory.learned_facts)\n",
        "            print(\"Conversations:\", len(memory.conversation_history))\n",
        "            continue\n",
        "\n",
        "        if user_input.strip():\n",
        "            response = smart_agent(user_input)\n",
        "            print(f\"ü§ñ Agent: {response}\")\n",
        "\n",
        "# Ready to test memory? Uncomment the line below:\n",
        "# chat_with_memory()\n",
        "\n",
        "print(\"\\nüéâ Your agent now has memory!\")\n",
        "print(\"üöÄ Next: Tutorial 4 - Take real actions\")"
      ]
    }
  ]
}