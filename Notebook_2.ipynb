{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORxl9OEBE7lfy1MYRozrNV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unverciftci/agents_intro/blob/main/Notebook_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "HDsoDGAVM1nn",
        "outputId": "4322ec69-cf88-413a-ea75-a1cd8ffc76d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3173483149.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load AI (same as Tutorial 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .utils import (\n\u001b[1;32m     29\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .auto_docstring import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mClassAttrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mClassDocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/auto_docstring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0m_prepare_output_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# required for @can_return_tuple decorator to work with torchdynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_debugging_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_addition_debugger_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2668\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   8014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8016\u001b[0;31m \u001b[0mactivate_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36mactivate_meta\u001b[0;34m()\u001b[0m\n\u001b[1;32m   7966\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_overload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpOverload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7968\u001b[0;31m         \u001b[0mop_overload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7970\u001b[0m         if torch._C._dispatch_has_kernel_for_dispatch_key(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 )\n\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_kernels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Tutorial 2: AI with Tools\n",
        "# Goal: Transform chatbot into tool-using agent\n",
        "\n",
        "import re\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load AI (same as Tutorial 1)\n",
        "print(\"Loading AI...\")\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"✅ AI ready!\")\n",
        "\n",
        "# Step 1: Create tools\n",
        "def calculator(expression):\n",
        "    \"\"\"Simple calculator tool\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return f\"Result: {result}\"\n",
        "    except:\n",
        "        return \"Error: Invalid calculation\"\n",
        "\n",
        "def get_weather(city):\n",
        "    \"\"\"Weather tool (simulated data)\"\"\"\n",
        "    weather_data = {\n",
        "        \"paris\": \"Sunny, 22°C\",\n",
        "        \"london\": \"Rainy, 15°C\",\n",
        "        \"tokyo\": \"Cloudy, 18°C\",\n",
        "        \"new york\": \"Windy, 20°C\"\n",
        "    }\n",
        "    return weather_data.get(city.lower(), \"Weather data not available\")\n",
        "\n",
        "# Tool registry\n",
        "TOOLS = {\n",
        "    \"calculator\": calculator,\n",
        "    \"get_weather\": get_weather\n",
        "}\n",
        "\n",
        "print(\"✅ Tools ready: calculator, get_weather\")\n",
        "\n",
        "# Step 2: Basic AI function\n",
        "def ask_ai(prompt):\n",
        "    \"\"\"Send prompt to AI and get response\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=120,\n",
        "            temperature=0.3,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "    return tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "# Step 3: Agent with tools\n",
        "def agent_with_tools(user_message):\n",
        "    \"\"\"AI agent that can use tools\"\"\"\n",
        "\n",
        "    # System prompt with examples\n",
        "    system_prompt = f\"\"\"You are an AI assistant that can use tools. When you need to:\n",
        "- Calculate something: use TOOL:calculator:expression\n",
        "- Get weather: use TOOL:get_weather:city\n",
        "\n",
        "Examples:\n",
        "User: What's 15 + 27?\n",
        "AI: I'll calculate that for you. TOOL:calculator:15+27\n",
        "\n",
        "User: What's the weather in Paris?\n",
        "AI: Let me check the weather. TOOL:get_weather:paris\n",
        "\n",
        "User: Hi there!\n",
        "AI: Hello! How can I help you today?\n",
        "\n",
        "User: {user_message}\n",
        "AI:\"\"\"\n",
        "\n",
        "    print(f\"👤 User: {user_message}\")\n",
        "\n",
        "    # Get AI response\n",
        "    ai_response = ask_ai(system_prompt)\n",
        "    print(f\"🤖 AI: {ai_response}\")\n",
        "\n",
        "    # Check if AI wants to use a tool\n",
        "    if \"TOOL:\" in ai_response:\n",
        "        tool_match = re.search(r'TOOL:(\\w+):(.+)', ai_response)\n",
        "        if tool_match:\n",
        "            tool_name = tool_match.group(1)\n",
        "            tool_input = tool_match.group(2).strip()\n",
        "\n",
        "            print(f\"🛠️ Using tool: {tool_name}({tool_input})\")\n",
        "\n",
        "            if tool_name in TOOLS:\n",
        "                tool_result = TOOLS[tool_name](tool_input)\n",
        "                print(f\"✅ Tool result: {tool_result}\")\n",
        "\n",
        "                # Get final response with tool result\n",
        "                final_prompt = system_prompt + ai_response + f\"\\nTool result: {tool_result}\\nFinal response:\"\n",
        "                final_response = ask_ai(final_prompt)\n",
        "\n",
        "                return f\"{ai_response}\\nTool result: {tool_result}\\n{final_response}\"\n",
        "            else:\n",
        "                return f\"{ai_response}\\nError: Unknown tool '{tool_name}'\"\n",
        "\n",
        "    return ai_response\n",
        "\n",
        "# Step 4: Test the agent\n",
        "print(\"\\n🧪 Testing agent with tools:\")\n",
        "\n",
        "test_questions = [\n",
        "    \"What's 25 * 4?\",\n",
        "    \"What's the weather in London?\",\n",
        "    \"Hello! How are you?\",\n",
        "    \"Calculate 100 / 5 please\"\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n--- Test ---\")\n",
        "    response = agent_with_tools(question)\n",
        "    print(f\"Final response: {response}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Step 5: Interactive mode\n",
        "def chat_with_agent():\n",
        "    \"\"\"Interactive chat with tool-using agent\"\"\"\n",
        "    print(\"\\n🛠️ Chat with your agent (now with tools!)\")\n",
        "    print(\"Try asking for calculations or weather!\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\n👤 You: \")\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"👋 Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip():\n",
        "            response = agent_with_tools(user_input)\n",
        "            print(f\"🤖 Final: {response}\")\n",
        "\n",
        "# Ready to test? Uncomment the line below:\n",
        "# chat_with_agent()\n",
        "\n",
        "print(\"\\n🎉 Your AI can now use tools!\")\n",
        "print(\"🚀 Next: Tutorial 3 - Add memory\")"
      ]
    }
  ]
}